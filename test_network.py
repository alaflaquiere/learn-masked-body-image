#!/usr/bin/env python
# coding: utf-8

import os
from argparse import ArgumentParser
import tensorflow as tf
import numpy as np
from sklearn import mixture
from scipy import interpolate
import cv2
from tools import Normalizer, create_checkerboard, load_data, load_network
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
plt.ion()


def reconstruct_training_data(dir_model="model/trained", dir_dataset="dataset/generated/combined", indexes=6):
    """
    Test a network by reconstructing samples from the dataset.

    Parameters:
        dir_model - model directory
        dir_dataset - dataset directory
        index - indexes (if list) or number of random indexes (if int) of samples to reconstruct
    """

    # load the dataset
    m, s, n_samples, height, width, n_channels, n_joints = load_data(dir_dataset)

    # draw indexes if necessary
    if type(indexes) == int:
        indexes = np.random.choice(n_samples, indexes)

    # normalize the motor_input configuration in [-1, 1] and subsample the dataset
    m_normalizer = Normalizer(low=-1, high=1)
    m = m_normalizer.fit_transform(m)
    m = m[indexes, :]

    # normalize the pixel channels in [0, 1] and subsample the dataset
    s_normalizer = Normalizer(low=0, high=1, min_data=0, max_data=1)  # identity mapping in this case, as the pixel values are already in [0, 1]
    s = s_normalizer.transform(s)
    s = s[indexes, :]

    # load the network
    saver, motor_input, predicted_image, predicted_error = load_network(dir_model)

    # create a background checkerboard
    checkerboard = create_checkerboard(height, width)

    # display the reconstructions
    with tf.Session() as sess:

        # reload the network's variable values
        saver.restore(sess, tf.train.latest_checkpoint(dir_model + "/"))

        for i, ind in enumerate(indexes):

            # ground truth image
            gt_image = s[i, :, :, :]

            # predict image
            curr_image = sess.run(predicted_image, feed_dict={motor_input: m[[i], :]})
            curr_image = curr_image[0]
            curr_image = s_normalizer.reconstruct(curr_image)  # identity mapping in this case, as the pixel values are already in [0, 1]

            # predict error
            curr_error = sess.run(predicted_error, feed_dict={motor_input: m[[i], :]})
            curr_error = curr_error[0]

            # build mask
            curr_mask = (curr_error <= 0.056).astype(float)

            # build the masked image
            curr_masked_image = np.dstack((curr_image, np.mean(curr_mask, axis=2)))

            # display
            fig = plt.figure(figsize=(12, 6))
            ax1 = fig.add_subplot(131)
            ax2 = fig.add_subplot(232)
            ax3 = fig.add_subplot(233)
            ax4 = fig.add_subplot(235)
            ax5 = fig.add_subplot(236)
            #
            fig.suptitle('sample {}'.format(ind), fontsize=12)
            #
            ax1.set_title("ground-truth image")
            ax1.imshow(gt_image)
            ax1.axis("off")
            #
            ax2.set_title("predicted image")
            ax2.imshow(curr_image)
            ax2.axis("off")
            #
            ax3.set_title("predicted error")
            ax3.imshow(curr_error)
            ax3.axis("off")
            #
            ax4.set_title("mask")
            ax4.imshow(curr_mask)
            ax4.axis("off")
            #
            ax5.set_title('masked predicted image')
            ax5.imshow(checkerboard)
            ax5.imshow(curr_masked_image)
            ax5.axis("off")

    plt.show(block=False)
    plt.pause(0.001)


def evaluate_body_image(dir_model="model/trained", dir_green_dataset="dataset/generated/green", indexes=6):
    """
    Test the body image mask generated by a network by comparing it the ground-truth green-background dataset.

    Parameters:
        dir_model - model directory
        dir_green_dataset - green-background dataset directory
        index - indexes (if list) or number of random indexes (if int) of samples to reconstruct
    """

    # load the dataset
    m, s, n_samples, height, width, n_channels, n_joints = load_data(dir_green_dataset)

    # draw indexes if necessary
    if type(indexes) == int:
        indexes = np.random.choice(n_samples, indexes)

    # normalize the motor_input configuration in [-1, 1] and subsample the dataset
    m_normalizer = Normalizer(low=-1, high=1)
    m = m_normalizer.fit_transform(m)

    # normalize the pixel channels in [0, 1] and subsample the dataset
    s_normalizer = Normalizer(low=0, high=1, min_data=0, max_data=1)  # identity mapping in this case, as the pixel values are already in [0, 1]
    s = s_normalizer.transform(s)

    # load the network
    saver, motor_input, predicted_image, predicted_error = load_network(dir_model)

    # create a background checkerboard
    checkerboard = create_checkerboard(height, width)

    # track all matches over the training set
    all_mask_match = []
    all_appearance_match = []

    with tf.Session() as sess:

        # reload the network's variable values
        saver.restore(sess, tf.train.latest_checkpoint(dir_model + "/"))

        # compute the mask and appearance matches over the whole dataset
        for ind in range(n_samples):

            # image with green background
            green_image = s[ind, :, :, :]

            # mask of the green background
            where_green = ((green_image[:, :, 0] == 0) & (abs(green_image[:, :, 1] - 141/255) <= 1e-3) & (green_image[:, :, 2] == 0)).astype(float)
            where_green = np.repeat(where_green[:, :, np.newaxis], 3, axis=2)  # copy on all three RGB channels

            # predict image
            curr_image = sess.run(predicted_image, feed_dict={motor_input: m[[ind], :]})
            curr_image = curr_image[0]
            curr_image = s_normalizer.reconstruct(curr_image)  # identity mapping in this case, as the pixel values are already in [0, 1]

            # predict error
            curr_error = sess.run(predicted_error, feed_dict={motor_input: m[[ind], :]})
            curr_error = curr_error[0]

            # build mask
            curr_mask = (curr_error <= 0.056).astype(float)

            # build the masked predicted image
            curr_masked_image = np.dstack((curr_image, np.mean(curr_mask, axis=2)))

            # build the masked green-background image
            curr_masked_green_image = np.dstack((green_image, np.mean(curr_mask, axis=2)))

            # error between green background mask and predicted mask
            error_mask_green = where_green - (1 - curr_mask)

            # matching between the two masks
            mask_match = 1 - np.sum(np.abs(error_mask_green)) / np.prod(error_mask_green.shape)

            # error between the arm appearance under the predicted mask
            image_error = np.abs(green_image - curr_image)
            image_error = image_error * curr_mask

            # matching between the appearances between the predicted mask
            if not np.sum(curr_mask) == 0:
                appearance_match = 1 - np.sum(np.abs(image_error)) / np.sum(curr_mask)
            else:
                appearance_match = 1

            # store the matches
            all_mask_match.append(mask_match)
            all_appearance_match.append(appearance_match)

            # display the matches for the selected indexes
            if ind in indexes:

                # display
                fig = plt.figure(figsize=(12, 6))
                ax1 = fig.add_subplot(231)
                ax2 = fig.add_subplot(232)
                ax3 = fig.add_subplot(233)
                ax4 = fig.add_subplot(234)
                ax5 = fig.add_subplot(235)
                ax6 = fig.add_subplot(236)
                #
                fig.suptitle('sample {}'.format(ind), fontsize=12)
                #
                ax1.set_title("ground-truth background")
                ax1.imshow(green_image * where_green)
                ax1.axis("off")
                #
                ax2.set_title("predicted mask")
                ax2.imshow(curr_mask)
                ax2.axis("off")
                #
                ax3.set_title("mask error: {:.2f}%".format(100 * mask_match), fontsize=11)
                ax3.imshow(error_mask_green / 2 + 0.5)
                ax3.axis("off")
                #
                #
                ax4.set_title("masked ground-truth")
                ax4.imshow(checkerboard)
                ax4.imshow(curr_masked_green_image)
                ax4.axis("off")
                #
                ax5.set_title("masked prediction")
                ax5.imshow(checkerboard)
                ax5.imshow(curr_masked_image)
                ax5.axis("off")
                #
                ax6.set_title("appearance error: {:2f}%".format(100 * appearance_match), fontsize=11)
                ax6.imshow(checkerboard)
                ax6.imshow(curr_masked_image)
                ax6.axis("off")

    # print the stats
    print("mask match = {mean} +/- {std}".format(mean=np.mean(all_mask_match), std=np.std(all_mask_match)))
    print("appearance match = {mean} +/- {std}".format(mean=np.mean(all_appearance_match), std=np.std(all_appearance_match)))

    plt.show(block=False)
    plt.pause(0.001)


def fit_gmm(dir_green_dataset="dataset/generated/green", dir_model="model/trained", indexes=100):
    """
    Fit a 2-Gaussian Mixture Model to the predicted prediction error distribution  over the whole dataset
    to distinguish the pixels belonging to the body image from the ones belonging to the background.

    Parameters:
        dir_dataset - dataset directory
        dir_model - model directory
        index - indexes (if list) or number of random indexes (if int) of samples to reconstruct
    """

    # load the dataset
    m, _, n_samples, _, _, _, _ = load_data(dir_green_dataset)

    # draw indexes if necessary
    if type(indexes) == int:
        indexes = np.random.choice(n_samples, indexes)

    # normalize the motor_input configuration in [-1, 1] and subsample the dataset
    m_normalizer = Normalizer(low=-1, high=1)
    m = m_normalizer.fit_transform(m)
    m = m[indexes, :]

    # load the network
    saver, motor_input, _, predicted_error = load_network(dir_model)

    # initialize list
    all_pred_errors = []

    # stack all the predicted prediction errors over the selected set of motor samples
    with tf.Session() as sess:

        # reload the network's variable values
        saver.restore(sess, tf.train.latest_checkpoint(dir_model + "/"))

        for i, ind in enumerate(indexes):

            # predict error
            curr_error = sess.run(predicted_error, feed_dict={motor_input: m[[i], :]})
            curr_error = curr_error[0]

            # append errors
            all_pred_errors = all_pred_errors + list(curr_error.flatten())

    # fit a 2-GMM model
    all_pred_errors = np.array(all_pred_errors).reshape(-1, 1)
    gmm_model = mixture.GaussianMixture(n_components=2, n_init=5)
    gmm_model.fit(all_pred_errors)

    # find the intersection of the two gaussians
    x = np.linspace(-0.05, 0.3, 1000).reshape(-1, 1)
    lp = gmm_model.score_samples(x)  # log probability
    p = gmm_model.predict_proba(x)  # class prediction
    diff = np.abs(p[:, 0] - p[:, 1])
    cross_index = np.argmin(diff)
    threshold = x[cross_index, 0]

    print("Estimated error threshold: {:.3f}".format(threshold))

    # display the histogram and optimizes gaussians
    fig = plt.figure()
    ax = fig.add_subplot(111)
    #
    ax.hist(all_pred_errors[:, 0], bins=100, normed=True, color="blue", rwidth=0.8, label="errors")
    ax.plot(x, np.exp(lp), 'r-', label="GMM")
    ax.legend(loc="upper left")
    #
    ax2 = ax.twinx()
    ax2.plot(x, p[:, 0], 'c--', label="Proba comp 1")
    ax2.plot(x, p[:, 1], 'g--', label="Proba comp 2")
    ax2.set_ylim([0, 1.2])
    ax2.legend(loc="upper right")
    #
    plt.show(block=False)
    plt.pause(0.001)

    return threshold


def explore_joint_space(dir_model="model/trained", motor_input_ref=None):
    """
    Regularly sample each dimension of the motor space and display the generated body image.

    Parameters:
        dir_model - model directory
        index - indexes (if list) or number of random indexes (if int) of samples to reconstruct
        motor_input_ref - reference motor input from which to explore the motor space
    """

    # load the network
    saver, motor_input, predicted_image, predicted_error = load_network(dir_model)

    # get parameters
    n_joints = motor_input.get_shape()[1].value
    height = predicted_image.get_shape()[1].value
    width = predicted_image.get_shape()[2].value

    # generate the reference motor input if necessary
    if motor_input_ref is None:
        motor_input_ref = np.zeros((1, n_joints))

    # create the sensory normalizer
    s_normalizer = Normalizer(low=0, high=1, min_data=0, max_data=1)  # identity mapping in this case, as the pixel values are already in [0, 1]

    # create a background checkerboard
    checkerboard = create_checkerboard(height, width)

    # display the reconstructions
    with tf.Session() as sess:

        # reload the network's variable values
        saver.restore(sess, tf.train.latest_checkpoint(dir_model + "/"))

        # iterate over the motor dimensions
        for joint in range(n_joints):

            # create a figure
            fig = plt.figure(figsize=(12, 6))

            for index, val in enumerate(np.linspace(-1, 1, 6)):

                # variation to add to the reference motor input
                delta = [[val if i == joint else 0. for i in range(n_joints)]]

                # predict image
                curr_image = sess.run(predicted_image, feed_dict={motor_input: motor_input_ref + delta})
                curr_image = curr_image[0]
                curr_image = s_normalizer.reconstruct(curr_image)  # identity mapping in this case, as the pixel values are already in [0, 1]

                # predict error
                curr_error = sess.run(predicted_error, feed_dict={motor_input: motor_input_ref + delta})
                curr_error = curr_error[0]

                # display
                ax1 = fig.add_subplot(2, 6, 1 + index)
                ax2 = fig.add_subplot(2, 6, 7 + index)
                #
                fig.suptitle('joint {}'.format(joint), fontsize=12)
                #
                ax1.set_title("predicted image")
                ax1.imshow(curr_image)
                ax1.axis("off")
                #
                ax2.set_title("predicted error")
                ax2.imshow(curr_error)
                ax2.axis("off")

    plt.show(block=False)
    plt.pause(0.001)


def generate_video(dir_model="model/trained", n_samples=2000, dir_video="temp/video"):
    """
    Generate of video of the estimated body image by randomly and smoothly moving in the motor space.

    Parameters:
        dir_model - model directory
        n_samples - number of samples in the motor space
        dir_video - directory where to save the video
    """

    # check the video directory
    if os.path.exists(dir_video):
        ans = input("> The folder {} already exists; do you want to overwrite its content? [y,n]: ".format(dir_video))
        if ans is not "y":
            print("exiting the program")
            return
    if not os.path.exists(dir_video):
        os.makedirs(dir_video)

    # normalize the pixel channels in [0, 1] and subsample the dataset
    s_normalizer = Normalizer(low=0, high=1, min_data=0, max_data=1)  # identity mapping in this case, as the pixel values are already in [0, 1]

    # load the network
    saver, motor_input, predicted_image, predicted_error = load_network(dir_model)

    # get parameters
    n_joints = motor_input.get_shape()[1].value
    height = predicted_image.get_shape()[1].value
    width = predicted_image.get_shape()[2].value

    # create a background checkerboard
    checkerboard = create_checkerboard(height, width)

    # create a smooth trajectory in the motor space
    n_anchors = n_samples//40
    anchors = 2 * np.random.rand(n_anchors, n_joints) - 1
    trajectory = np.full((n_samples, n_joints), np.nan)
    for k in range(4):
        tck = interpolate.splrep(np.linspace(0, 1, n_anchors), anchors[:, k])
        trajectory[:, k] = interpolate.splev(np.linspace(0, 1, n_samples), tck)

    # prepare the video writer
    video = cv2.VideoWriter(filename=dir_video + "/video.avi", fourcc=cv2.VideoWriter_fourcc(*'XVID'), fps=24, frameSize=(800, 600))

    # prepare the figure
    fig = plt.figure(figsize=(8, 6))
    ax0 = fig.add_subplot(231, projection="3d")
    ax1 = fig.add_subplot(234, projection="3d")
    ax2 = fig.add_subplot(232)
    ax3 = fig.add_subplot(233)
    ax4 = fig.add_subplot(235)
    ax5 = fig.add_subplot(236)

    with tf.Session() as sess:

        # reload the network's variable values
        saver.restore(sess, tf.train.latest_checkpoint(dir_model + "/"))

        for k in range(n_samples):

            print("\rframe {}".format(k, end=""))

            # get the motor input
            curr_motor = trajectory[[k], :]

            # predict image
            curr_image = sess.run(predicted_image, feed_dict={motor_input: curr_motor})
            curr_image = curr_image[0]
            curr_image = s_normalizer.reconstruct(curr_image)  # identity mapping in this case, as the pixel values are already in [0, 1]

            # predict error
            curr_error = sess.run(predicted_error, feed_dict={motor_input: curr_motor})
            curr_error = curr_error[0]

            # build mask
            curr_mask = (curr_error <= 0.056).astype(float)

            # build the masked image
            curr_masked_image = np.dstack((curr_image, np.mean(curr_mask, axis=2)))

            # display the motor configuration with a trace
            ax0.cla()
            ax0.set_title('$m_1, m_2, m_3$')
            ax0.plot(trajectory[max(0, k - 48):k, 0], trajectory[max(0, k - 48):k, 1], trajectory[max(0, k - 48):k, 2], 'b-')
            ax0.plot(trajectory[k - 1:k, 0], trajectory[k - 1:k, 1], trajectory[k - 1:k, 2], 'ro')
            ax0.set_xlim(-1, 1)
            ax0.set_ylim(-1, 1)
            ax0.set_zlim(-1, 1)
            ax0.set_xticklabels([])
            ax0.set_yticklabels([])
            ax0.set_zticklabels([])
            #
            ax1.cla()
            ax1.set_title('$m_2, m_3, m_4$')
            ax1.plot(trajectory[max(0, k - 48):k, 1], trajectory[max(0, k - 48):k, 2], trajectory[max(0, k - 48):k, 3], 'b-')
            ax1.plot(trajectory[k - 1:k, 1], trajectory[k - 1:k, 2], trajectory[k - 1:k, 3], 'ro')
            ax1.set_xlim(-1, 1)
            ax1.set_ylim(-1, 1)
            ax1.set_zlim(-1, 1)
            ax1.set_xticklabels([])
            ax1.set_yticklabels([])
            ax1.set_zticklabels([])

            # display the predicted image
            ax2.cla()
            ax2.set_title("predicted image")
            ax2.imshow(curr_image)
            ax2.axis("off")
            #
            ax3.cla()
            ax3.set_title("predicted error")
            ax3.imshow(curr_error)
            ax3.axis("off")
            #
            ax4.cla()
            ax4.set_title("predicted mask")
            ax4.imshow(curr_mask)
            ax4.axis("off")
            #
            ax5.cla()
            ax5.set_title("masked prediction")
            ax5.imshow(checkerboard)
            ax5.imshow(curr_masked_image)
            ax5.axis("off")

            plt.show(block=False)
            fig.savefig(dir_video + "/img.png")
            plt.pause(0.001)

            # write frame
            image = cv2.imread(dir_video + "/img.png")
            video.write(image)

    # clean up
    cv2.destroyAllWindows()
    video.release()
    os.remove(dir_video + "/img.png")


if __name__ == "__main__":

    parser = ArgumentParser()
    parser.add_argument("-dm", "--dir_model", dest="dir_model", help="path to the model", default=".model/trained")
    parser.add_argument("-dd", "--dir_dataset", dest="dir_dataset", help="path to training dataset", default=".dataset/generated/combined")
    parser.add_argument("-dg", "--dir_green", dest="dir_green_dataset", help="path to training dataset with green background", default=".dataset/generated/green")
    parser.add_argument("-dv", "--dir_video", dest="dir_video", help="directory to save the video", default=".temp/video")

    args = parser.parse_args()
    dir_model = args.dir_model
    dir_dataset = args.dir_dataset
    dir_green_dataset = args.dir_green_dataset
    dir_video = args.dir_video

    reconstruct_training_data(dir_model=dir_model, dir_dataset=dir_dataset, indexes=3)
    evaluate_body_image(dir_model=dir_model, dir_green_dataset=dir_green_dataset, indexes=3)
    fit_gmm(dir_green_dataset=dir_green_dataset, dir_model=dir_model, indexes=100)
    explore_joint_space(dir_model=dir_model)
    generate_video(dir_model=dir_model, dir_video=dir_video)

    print("testing finished.")
    plt.show(block=True)
